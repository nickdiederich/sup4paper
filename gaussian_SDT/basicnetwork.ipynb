{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Network details: \n",
    "#                     !!!    pictures=4    !!! \n",
    "# ================\n",
    "# this network is designed to decorrelate 4 pattern:\n",
    "#         if you want to train only X pattern you only have to change\n",
    "#            # number of training pictures: pictures=4 to pictures=X \n",
    "#         than the third training pattern will acts as test pattern only\n",
    "#         \n",
    "# the network will decorellate the training pattern\n",
    "# the network has a correction term to conserve the number of active outputs (more or less)\n",
    "# the network is made sensitive in weights to be able to decorrelate for never known inputs \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Current processing parameters:\n",
    "    #42% continuous noise via Pixel Selection\n",
    "    #25 training runs\n",
    "    #100 subjects, 100 recall images\n",
    "    #Lures via blending with equally distributed input ratios are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot \n",
    "# ensure the plots are inside this notebook, not an external window\n",
    "%matplotlib inline\n",
    "import scipy as sc\n",
    "from scipy import sparse\n",
    "np.set_printoptions(precision=4)\n",
    "import random\n",
    "#--------------------------\n",
    "# Set the number of change pixels\n",
    "number_of_change_pixels=108 \n",
    "rsaw=np.array(range(0,256)) #index of input array, will randomized by shuffel in every loop \n",
    "print(\"pixelselection rate =\", (100*number_of_change_pixels/256),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorrelationNetwork\n",
    "class decorrelationNetwork:\n",
    "    \n",
    "    # initialise\n",
    "    def __init__(self, inputnodes, outputnodes, sparsness, threshold_val, lerningrate):\n",
    "        # set number of in- and outputnodes\n",
    "        self.innodes=inputnodes\n",
    "        self.outnodes=outputnodes\n",
    "        self.sparsity=sparsness/2\n",
    "\n",
    "        # lenght of inputpixel; quadratic size only\n",
    "        self.__length=np.sqrt(self.innodes).astype(int)\n",
    "        \n",
    "        # threshold of output neurons\n",
    "        self.th=threshold_val\n",
    "        \n",
    "        # initial matrix:\n",
    "        # sparse random matrix\n",
    "        self.woi=sc.sparse.rand(self.outnodes,self.innodes,self.sparsity,'coo',np.float64).toarray()-sc.sparse.rand(self.outnodes,self.innodes,self.sparsity,'coo',np.float64).toarray()\n",
    "\n",
    "        #set lerningrate\n",
    "        self.lr=lerningrate\n",
    "        # activation function is heavyside function, lambda generates function\n",
    "        self.activation_function = lambda x: np.heaviside(x-self.th,1)\n",
    "        pass\n",
    "# ========================================================================================================        \n",
    "    def overwrite(self, inputnodes, outputnodes, sparsness, threshold_val, lerningrate):\n",
    "        # set number of in- and outputnodes\n",
    "        self.innodes=inputnodes\n",
    "        self.outnodes=outputnodes\n",
    "        self.sparsity=sparsness/2\n",
    "\n",
    "        # lenght of inputpixel; quadratic size only\n",
    "        self.__length=np.sqrt(self.innodes).astype(int)\n",
    "        \n",
    "        # threshold of output neurons\n",
    "        self.th=threshold_val\n",
    "        \n",
    "        # initial matrix:\n",
    "        # sparse random matrix\n",
    "        self.woi=sc.sparse.rand(self.outnodes,self.innodes,self.sparsity,'coo',np.float64).toarray()-sc.sparse.rand(self.outnodes,self.innodes,self.sparsity,'coo',np.float64).toarray()\n",
    "\n",
    "        #set lerningrate\n",
    "        self.lr=lerningrate\n",
    "        # activation function is heavyside function, lambda generates function\n",
    "        self.activation_function = lambda x: np.heaviside(x-self.th,1)\n",
    "        pass\n",
    "# ========================================================================================================    \n",
    "    # training\n",
    "    def train(self, input_list, sensivity, thresholdadaption):\n",
    "        # convert input list to 2d array\n",
    "        # noise term for SDT otherwise the corrcoeff for known images =1 (added in training for consistency)\n",
    "        # blending (unused): noise_input_list=(input_list+input_variation*np.random.uniform(0,1,input_nodes))/(1.0+input_variation)\n",
    "        # but for pixelselection:\n",
    "        noise_input_list=input_list.copy()\n",
    "        random.shuffle(rsaw) \n",
    "        for i in range(number_of_change_pixels):\n",
    "            noise_input_list[rsaw[i]]=np.random.uniform(0,1,1)\n",
    "            pass\n",
    "        # --------------\n",
    "        inputs=np.array(noise_input_list, ndmin=2).T\n",
    "        # input to secound (output) layer\n",
    "        sec_in=np.dot(self.woi, inputs)\n",
    "        # addaption of threshold is equivalent manipulate input to outputneuron\n",
    "        thres_adaption=np.array(thresholdadaption, ndmin=2).T\n",
    "        sec_input=np.add(sec_in,-thres_adaption)\n",
    "        # calculate the output\n",
    "        orthogonal_output=self.activation_function(sec_input)\n",
    "        # =================================================================================================\n",
    "        # (in)activate outputneurons of skiepattern is to far away from target number\n",
    "        # activate \n",
    "        activations=(self.outnodes/4-sum(orthogonal_output))\n",
    "        activations=np.int(activations) # .astype(int)\n",
    "        inactivations=0\n",
    "        if activations>10:       \n",
    "            # randomly searching for inactiv outputneurons \n",
    "            chosen_neuron=np.random.randint(self.outnodes)\n",
    "            while orthogonal_output[chosen_neuron]!=0:\n",
    "                chosen_neuron=np.random.randint(self.outnodes)\n",
    "                pass\n",
    "            # build or increase connection unless the nuron is activated\n",
    "            while orthogonal_output[chosen_neuron]==0:\n",
    "                chosen_input=np.random.randint(self.innodes)\n",
    "                while inputs[chosen_input]>(0.66+np.random.rand()/3):\n",
    "                    chosen_input=np.random.randint(self.innodes)\n",
    "                    pass\n",
    "                self.woi[chosen_neuron][chosen_input]+=2.0*np.random.rand()\n",
    "                sec_input=np.dot(self.woi, inputs)\n",
    "                orthogonal_output=self.activation_function(sec_input)\n",
    "            pass       \n",
    "        elif activations<0:\n",
    "            inactivations=-activations\n",
    "            pass\n",
    "        # ================   \n",
    "        #decrease synaptic connetion or cut synaptic connetion if it is small -> to conserve sparsness\n",
    "        if inactivations>10:\n",
    "            # randomly searching for inactiv outputneurons \n",
    "            chosen_neuron=np.random.randint(self.outnodes)\n",
    "            while orthogonal_output[chosen_neuron]!=1:\n",
    "                chosen_neuron=np.random.randint(self.outnodes)\n",
    "                pass\n",
    "            # searching for small input values with statistical choise\n",
    "            chosen_input=np.random.randint(self.innodes)\n",
    "            while inputs[chosen_input]<(0.25-np.random.rand()/4):\n",
    "                chosen_input=np.random.randint(self.innodes)\n",
    "                pass \n",
    "            # cut synaptic connetion if it is small -> to conserve sparsness\n",
    "            if (self.woi[chosen_neuron][chosen_input]>-sensivity and self.woi[chosen_neuron][chosen_input]<sensivity):\n",
    "                self.woi[chosen_neuron][chosen_input]=0.0\n",
    "            # or increase\n",
    "            else:\n",
    "                while orthogonal_output[chosen_neuron]==1:\n",
    "                    self.woi[chosen_neuron][chosen_input]-=0.01*self.lr\n",
    "                    sec_input=np.dot(self.woi, inputs)\n",
    "                    orthogonal_output=self.activation_function(sec_input)\n",
    "                    pass                \n",
    "                pass\n",
    "            pass      \n",
    "        # ===============================================================================================\n",
    "        # new calculation of output after manipulation\n",
    "        orthogonal_output=self.activation_function(sec_input)\n",
    "        #make the system more sensitive for new inputs\n",
    "        randomize_neuron=np.random.randint(self.outnodes)  \n",
    "        while orthogonal_output[randomize_neuron]!=0:\n",
    "            randomize_neuron=np.random.randint(self.outnodes)\n",
    "            pass    \n",
    "        while (sec_input[randomize_neuron]+thres_adaption[randomize_neuron])>(self.th+sensivity):\n",
    "            self.woi[randomize_neuron][:]*=0.95\n",
    "            sec_input=np.dot(self.woi, inputs)\n",
    "            pass\n",
    "        randomize_neuron_inact=np.random.randint(self.outnodes)  \n",
    "        while orthogonal_output[randomize_neuron_inact]!=1:\n",
    "            randomize_neuron_inact=np.random.randint(self.outnodes)\n",
    "            pass    \n",
    "        while (sec_input[randomize_neuron_inact]+thres_adaption[randomize_neuron_inact])<(self.th-sensivity):\n",
    "            self.woi[randomize_neuron_inact][:]*=1.05\n",
    "            sec_input=np.dot(self.woi, inputs)\n",
    "            pass\n",
    "        return self.woi        \n",
    "# ========================================================================================================     \n",
    "    # Results\n",
    "    def query(self, input_list, thresholdadaption):\n",
    "        # convert input list to 2d array (was hier mit 2d gemeint ist und warum das \n",
    "        # konvertiert werden muss ist mir nicht klar, .T steht jedenfalls fuer Transponieren)\n",
    "        # Rauschterm für SDT sonst ist der corrcoeff für bekannnte Bilder =1\n",
    "        # blending: noise_input_list=(input_list+input_variation*np.random.uniform(0,1,input_nodes))/(1.0+input_variation)\n",
    "        # doch fuer Pixelselection:\n",
    "        noise_input_list=input_list.copy()\n",
    "        random.shuffle(rsaw) \n",
    "        for i in range(number_of_change_pixels):\n",
    "            noise_input_list[rsaw[i]]=np.random.uniform(0,1,1)\n",
    "            pass\n",
    "        # --------------\n",
    "        inputs=np.array(noise_input_list, ndmin=2).T\n",
    "        # input to secound (output) layer\n",
    "        sec_in=np.dot(self.woi, inputs)\n",
    "        # addaption of threshold is equivalent manipulate input\n",
    "        thres_adaption=np.array(thresholdadaption, ndmin=2).T\n",
    "        sec_input=np.add(sec_in,-thres_adaption)\n",
    "        orthogonal_output=self.activation_function(sec_input)\n",
    "        return orthogonal_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameter to initilise\n",
    "\n",
    "# number of training pictures\n",
    "pictures=4\n",
    "# number of in- outputnodes and lerningrate\n",
    "input_nodes=256\n",
    "length_in=np.sqrt(input_nodes).astype(int)\n",
    "output_nodes=1024\n",
    "length_out=np.sqrt(output_nodes).astype(int)\n",
    "lerning_rate=0.3\n",
    "\n",
    "# sparseness of random connections from EC to DG (perforant path) \n",
    "sparse_factor=0.05\n",
    "\n",
    "# threshold of output neurons\n",
    "threshold=0.85\n",
    "sensibility_factor=0.15 \n",
    "threshold_correction=np.zeros(output_nodes)\n",
    "\n",
    "# create instance of decorrelation network\n",
    "dn=decorrelationNetwork(input_nodes, output_nodes, sparse_factor, threshold, lerning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probanden=100   #number of probands\n",
    "Proband_No=0    #index of probands starting at null\n",
    "for prob in range(probanden):  \n",
    "    dn.overwrite(input_nodes, output_nodes, sparse_factor, threshold, lerning_rate)\n",
    "    # sparseness of random connections\n",
    "    sparse_factor=0.05\n",
    "\n",
    "    # threshold of output neurons\n",
    "    threshold=0.85\n",
    "    sensibility_factor=0.15 \n",
    "    threshold_correction=np.zeros(output_nodes)\n",
    "    # create training data with correlations <0.1\n",
    "    # create faulty matrix so that it goes into the loop\n",
    "    input_correlation=np.ones((5,5))\n",
    "    Proband_No+=1\n",
    "    print(Proband_No)\n",
    "    while (np.abs(input_correlation[0,1])>0.1 or np.abs(input_correlation[0,2])>0.1 or np.abs(input_correlation[0,3])>0.1 or np.abs(input_correlation[1,2])>0.1 or np.abs(input_correlation[1,3])>0.1 or np.abs(input_correlation[2,3])>0.1 ):\n",
    "        data_01=np.random.uniform(0,1,input_nodes)\n",
    "        data_02=np.random.uniform(0,1,input_nodes)\n",
    "        data_03=np.random.uniform(0,1,input_nodes)\n",
    "        data_04=np.random.uniform(0,1,input_nodes)\n",
    "        indat = [np.array(data_01), np.array(data_02), np.array(data_03), np.array(data_04)]\n",
    "        input_data = np.array(indat)\n",
    "        input_correlation=np.corrcoef(input_data)\n",
    "        #input_correlation\n",
    "        pass\n",
    "    # train the neural network and show sparsness of trained matrix\n",
    "    # epochs is the number of times the training data set is used for training\n",
    "    epochs = 25\n",
    "    for e in range(epochs):\n",
    "        total_output=np.zeros(output_nodes).T\n",
    "        for p in range(pictures):\n",
    "            woi=dn.train(input_data[p], sensibility_factor, threshold_correction)\n",
    "            total_output=np.add(total_output,dn.query(input_data[p], threshold_correction).T)\n",
    "            pass \n",
    "        for pix in range (output_nodes):\n",
    "            if total_output[0][pix]>np.random.randint(1,np.max(total_output)) and threshold_correction[pix]<1.5:\n",
    "                threshold_correction[pix]+=0.02*total_output[0][pix]*np.random.rand()\n",
    "                pass\n",
    "            pass\n",
    "        pass \n",
    "        \n",
    "    final_woi=woi.copy()\n",
    "    np.place(final_woi, final_woi>0, 1)\n",
    "    np.place(final_woi, final_woi<0, 1)\n",
    "    sum_colum=sum(final_woi)\n",
    "    final_sparsness=sum(sum_colum)/(input_nodes*output_nodes)\n",
    "    \n",
    "    druchlaeufe=25 # number of runs \n",
    "    fo = open(\"file.txt\", \"a\")\n",
    "    for durchl in range(druchlaeufe):\n",
    "        noise_pattern=np.random.uniform(0,1,input_nodes)\n",
    "        tastschritte=7 #sampling rate= 1/Tastschritte \n",
    "        for ts in range(tastschritte+1):\n",
    "            # convert for equal correlation steps:\n",
    "            if ts==0:\n",
    "                tsblq=0\n",
    "                pass\n",
    "            if ts>0:\n",
    "                tsq=ts*(1/(tastschritte)) #equidistant distances\n",
    "                tsblq=1/(1+np.sqrt((1/np.power(tsq,2))-1))  #equidistant distances when blending is taken into account\n",
    "                pass\n",
    "            testdata_a=tsblq*input_data[0].copy()+(1-tsblq)*noise_pattern.copy()\n",
    "            testdata_b=tsblq*input_data[1].copy()+(1-tsblq)*noise_pattern.copy()\n",
    "            testdata_c=tsblq*input_data[2].copy()+(1-tsblq)*noise_pattern.copy()\n",
    "            testdata_d=tsblq*input_data[3].copy()+(1-tsblq)*noise_pattern.copy()\n",
    "            testout_a=dn.query(testdata_a, threshold_correction)\n",
    "            testout_b=dn.query(testdata_b, threshold_correction)\n",
    "            testout_c=dn.query(testdata_c, threshold_correction)\n",
    "            testout_d=dn.query(testdata_d, threshold_correction)\n",
    "            if ts==0:\n",
    "                testoutput_a=testout_a.T\n",
    "                testoutput_b=testout_b.T\n",
    "                testoutput_c=testout_c.T\n",
    "                testoutput_d=testout_d.T\n",
    "                pass\n",
    "            if ts>0:\n",
    "                testoutput_a=np.append(testoutput_a, testout_a.T, axis=0)\n",
    "                testoutput_b=np.append(testoutput_b, testout_b.T, axis=0)\n",
    "                testoutput_c=np.append(testoutput_c, testout_c.T, axis=0)\n",
    "                testoutput_d=np.append(testoutput_d, testout_d.T, axis=0)\n",
    "        \n",
    "                pass\n",
    "            pass \n",
    "        #Create comparison pattern:\n",
    "        testout_a=dn.query(input_data[0], threshold_correction)\n",
    "        testout_b=dn.query(input_data[1], threshold_correction)\n",
    "        testout_c=dn.query(input_data[2], threshold_correction)\n",
    "        testout_d=dn.query(input_data[3], threshold_correction)\n",
    "        testoutput_a=np.append(testoutput_a, testout_a.T, axis=0)\n",
    "        testoutput_b=np.append(testoutput_b, testout_b.T, axis=0)\n",
    "        testoutput_c=np.append(testoutput_c, testout_c.T, axis=0)\n",
    "        testoutput_d=np.append(testoutput_d, testout_d.T, axis=0)\n",
    "        #compute correlations:\n",
    "        output_correlation_01=np.corrcoef(testoutput_a)\n",
    "        output_correlation_02=np.corrcoef(testoutput_b)\n",
    "        output_correlation_03=np.corrcoef(testoutput_c)\n",
    "        output_correlation_04=np.corrcoef(testoutput_d)\n",
    " \n",
    "        fo.write('\\n%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf' %(output_correlation_01[7,0],output_correlation_01[7,1],output_correlation_01[7,2],output_correlation_01[7,3],output_correlation_01[7,4],output_correlation_01[7,5],output_correlation_01[7,6]))\n",
    "        fo.write('\\n%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf' %(output_correlation_02[7,0],output_correlation_02[7,1],output_correlation_02[7,2],output_correlation_02[7,3],output_correlation_02[7,4],output_correlation_02[7,5],output_correlation_02[7,6]))  \n",
    "        fo.write('\\n%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf' %(output_correlation_03[7,0],output_correlation_03[7,1],output_correlation_03[7,2],output_correlation_03[7,3],output_correlation_03[7,4],output_correlation_03[7,5],output_correlation_03[7,6]))  \n",
    "        fo.write('\\n%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf\\t%.4lf' %(output_correlation_04[7,0],output_correlation_04[7,1],output_correlation_04[7,2],output_correlation_04[7,3],output_correlation_04[7,4],output_correlation_04[7,5],output_correlation_04[7,6]))  \n",
    "        pass\n",
    "    fo.close()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show input pattern 1\n",
    "image_array_01 = input_data[0].reshape((length_in,length_in)) \n",
    "image_array_02 = input_data[1].reshape((length_in,length_in))\n",
    "image_array_03 = input_data[2].reshape((length_in,length_in)) \n",
    "image_array_04 = input_data[3].reshape((length_in,length_in))\n",
    "matplotlib.pyplot.subplot(4, 4, 1)\n",
    "matplotlib.pyplot.imshow(image_array_01, cmap='Greys', interpolation='None')\n",
    "matplotlib.pyplot.subplot(4, 4, 2)\n",
    "matplotlib.pyplot.imshow(image_array_02, cmap='Greys', interpolation='None')\n",
    "matplotlib.pyplot.subplot(4, 4, 3)\n",
    "matplotlib.pyplot.imshow(image_array_03, cmap='Greys', interpolation='None')\n",
    "matplotlib.pyplot.subplot(4, 4, 4)\n",
    "matplotlib.pyplot.imshow(image_array_04, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a EC pattern (continous)\n",
    "matplotlib.pyplot.imshow(image_array_01, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a DG pattern (binary)\n",
    "image_array_02 = testoutput_a[6].reshape((32,32))\n",
    "matplotlib.pyplot.imshow(image_array_02, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
